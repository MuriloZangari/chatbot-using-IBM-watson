import gradio as gr
from cli_chatbot.watson_client import ask_watson, valida_resposta, model
from cli_chatbot.chat_history import get_context

# Estado global da conversa (interno ao modelo)
chat_history = get_context()

model_name = model.model_id.split("/")[-1]  # Extrai o nome do modelo para exibir na interface

def chatbot_interface(pergunta, history_ui):
    """
    Manipula a entrada do usu√°rio, envia para o modelo Watsonx,
    valida a resposta e atualiza o hist√≥rico exibido na interface.
    """
    global chat_history

    # Chamada ao modelo
    resposta = ask_watson(pergunta, chat_history)

    # Atualiza hist√≥rico interno
    chat_history.append(f"Usu√°rio: {pergunta}")
    chat_history.append(f"Assistente: {resposta['resposta']}")

    # Valida a resposta
    alertas = valida_resposta(resposta['resposta'])
    if alertas:
        resposta['resposta'] += f"\n\n‚ö†Ô∏è " + "\n".join(alertas)

    # Atualiza hist√≥rico da interface Gradio
    history_ui.append({"role": "user", "content": pergunta})
    history_ui.append({"role": "assistant", "content": resposta['resposta']})

    return "", history_ui

def limpar_chat():
    """
    Limpa apenas o hist√≥rico exibido na interface (n√£o reseta o chat_history).
    """
    return []

def ver_contexto():
    """
    Mostra o conte√∫do atual do contexto interno usado no modelo.
    """
    context = get_context()
    output = "\n".join(context)
    return [{"role": "assistant", "content": f"üìÑ Contexto inicial (mini-RAG):\n\n{output}"}]

def ver_prompt_base():
    """
    L√™ e exibe o conte√∫do do prompt base como mensagem do assistente.
    """
    try:
        with open("prompts/base_prompt.txt", "r") as f:
            prompt_text = f.read()
    except FileNotFoundError:
        prompt_text = "‚ö†Ô∏è Arquivo 'base_prompt.txt' n√£o encontrado."

    return [{"role": "assistant", "content": f"üìÑ **Prompt-base usado no modelo:**\n\n{prompt_text}"}]


# Interface Gradio
with gr.Blocks() as demo:
    gr.Markdown("# üí¨ Chatbot de Financiamento de Ve√≠culos (Watsonx.ai)")
    gr.Markdown("Digite sua pergunta sobre financiamento, parcelamento ou juros. Ex: `Quero financiar R$ 50.000 em 48x com 1,5% de juros ao m√™s.`")
    gr.Markdown(f"### Modelo: `{model_name}`")
    
    chatbot = gr.Chatbot(type="messages")

    with gr.Row():
        msg = gr.Textbox(
            placeholder="Digite sua pergunta aqui...",
            show_label=False,
            scale=8  # ocupa 80% da largura
        )
        enviar = gr.Button("üì® Enviar", scale=2)

    with gr.Row():
        clear = gr.Button("üîÑ Limpar conversa")
        contexto = gr.Button("üìÑ Ver contexto")
        prompt_base = gr.Button("üìÑ Ver prompt-base")


    # Enviar pergunta com Enter ou bot√£o
    msg.submit(fn=chatbot_interface, inputs=[msg, chatbot], outputs=[msg, chatbot])
    enviar.click(fn=chatbot_interface, inputs=[msg, chatbot], outputs=[msg, chatbot])

    # Limpar interface
    clear.click(fn=limpar_chat, outputs=[chatbot])

    # Ver contexto (mini-RAG)
    contexto.click(fn=ver_contexto, outputs=[chatbot])

    # Ver prompt-base
    prompt_base.click(fn=ver_prompt_base, outputs=[chatbot])

# Executa servidor local e gera link p√∫blico
if __name__ == "__main__":
    demo.launch(share=True)
