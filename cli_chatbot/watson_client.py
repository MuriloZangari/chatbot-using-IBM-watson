import os
from dotenv import load_dotenv
from ibm_watsonx_ai.foundation_models import ModelInference
from ibm_watsonx_ai import Credentials

load_dotenv()

API_KEY = os.getenv("WATSONX_API_KEY")
PROJECT_ID = os.getenv("WATSONX_PROJECT_ID")

creds = Credentials(
    url="https://us-south.ml.cloud.ibm.com",  # Dallas Region
    api_key=API_KEY
)

model = ModelInference(
    model_id="ibm/granite-3-3-8b-instruct",
    credentials=creds,
    project_id=PROJECT_ID
)

def get_default_params():
    """
    Retorna os par√¢metros de gera√ß√£o padr√£o compat√≠veis com o modelo Granite.
    Comentado para indicar quais s√£o obrigat√≥rios vs opcionais.

    Refer√™ncia: IBM Watsonx.ai SDK (ModelInference.generate)
    """

    return {
        "decoding_method": "greedy",        # ‚úÖ Obrigat√≥rio (greedy, sample ou beam)
        "max_new_tokens": 2000,              # ‚úÖ Obrigat√≥rio (limite m√°ximo de tokens gerados na resposta)

        "temperature": 0,                   # üîπ Opcional (default = 1.0). Controla a aleatoriedade. 0 = determin√≠stico
        "top_p": 1.0,                       # üîπ Opcional (default = 1.0). Nucleus sampling ‚Äî define a faixa de probabilidade acumulada
        "repetition_penalty": 1.0,          # üîπ Opcional (default = 1.0). Penaliza repeti√ß√µes se > 1.0

        "stop_sequences": ["Usu√°rio:"]       # üîπ Opcional. For√ßa o modelo a parar ao encontrar a sequ√™ncia especificada
    }


def ask_watson(pergunta, chat_history=None):
    BASE_PROMPT_PATH = os.path.join(os.path.dirname(__file__), "..", "prompts", "base_prompt.txt")
    system_prompt = open(BASE_PROMPT_PATH, encoding="utf-8").read()

    # Junta o hist√≥rico da conversa, se houver
    history_text = "\n".join(chat_history) if chat_history else ""

    # Monta o prompt com contexto
    full_prompt = (
        f"{system_prompt}\n\n"
        f"{history_text}\n"
        f"Usu√°rio: {pergunta}\n"
        f"Assistente:"
    )

    # Gera√ß√£o da resposta com os par√¢metros padr√£o
    response = model.generate(
        prompt=full_prompt,
        params=get_default_params()
    )

    text = response['results'][0]['generated_text'].strip()    

    if "Usu√°rio:" in text:
        # Remove a parte que simula o usu√°rio
        text = text.split("Usu√°rio:")[0].strip()

    return {
        "resposta": text
    }

def valida_resposta(texto):
    """
    Faz uma an√°lise leve da resposta gerada pelo modelo para detectar poss√≠veis alucina√ß√µes ou inconsist√™ncias.
    Retorna uma lista de alertas, se houver.
    """
    alertas = []

    # Heur√≠stica 1: men√ß√£o a leis, resolu√ß√µes ou normas
    if any(term in texto.lower() for term in ["lei", "resolu√ß√£o", "norma", "constitui√ß√£o"]):
        alertas.append("‚ö†Ô∏è A resposta menciona normas legais. Verifique se a refer√™ncia √© verdadeira.")

    # Heur√≠stica 2: m√∫ltiplas moedas muito discrepantes
    if texto.count("R$") >= 2:
        try:
            valores = [float(v.replace("R$", "").replace(".", "").replace(",", "."))
                       for v in texto.split() if "R$" in v]
            if len(valores) >= 2:
                maior = max(valores)
                menor = min(valores)
                if menor > 0 and maior / menor > 10:
                    alertas.append("‚ö†Ô∏è O valor total parece desproporcional. Verifique os c√°lculos.")
        except Exception:
            pass

    # Heur√≠stica 3: simula√ß√£o de turnos de conversa
    if "Usu√°rio:" in texto:
        alertas.append("‚ö†Ô∏è O modelo est√° simulando falas do usu√°rio. Isso deve ser evitado.")

    return alertas
